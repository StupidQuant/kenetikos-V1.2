# Technical Research Directive: Kinētikós Entropḗ v2.0 Engine

**Objective:** To generate a series of comprehensive technical reports providing the complete mathematical foundations and production-ready TypeScript/JavaScript implementations required to evolve the "kinētikós entropḗ" system from a v1.0 prototype into a v2.0 adaptive quantitative engine. This directive is divided into four sections, each corresponding to a critical development pillar.

---

### **Section 1: Causal Signal Processing**

**Objective:** To establish a methodologically sound foundation for all subsequent calculations by ensuring all signal processing is strictly causal (i.e., uses no future information).

**Part 1.A: Causal Savitzky-Golay Differentiation**
**Requirement:** A technical report detailing the implementation of a one-sided, backward-looking Savitzky-Golay filter.
*   **Mathematical Specification:** Provide the complete mathematical derivation and formulas for the one-sided Savitzky-Golay filter coefficients. These coefficients are used to compute a smoothed value and its first derivative at the most recent time point `t` using a window of past data `[t-n, ..., t]`.
*   **Implementation:** Deliver a production-ready, dependency-free TypeScript function with the signature `savitzkyGolayCausal(data: number[], windowSize: number, polynomialOrder: number): { smoothedValue: number, derivative: number }`. The function must be optimized for performance in a real-time environment.
*   **Comparative Analysis:** Include a critique comparing this method to simpler derivative estimation techniques (e.g., first-difference of an Exponential Moving Average), focusing on phase lag, noise suppression, and frequency response characteristics.

**Part 1.B: Causal and Adaptive Histogram Binning**
**Requirement:** A report on statistically robust methods for discretizing a rolling window of time-series data without look-ahead bias.
*   **Problem Formulation:** Formally explain why using a global `min` and `max` from an entire dataset to define histogram bins for a rolling calculation introduces look-ahead bias and invalidates the analysis for backtesting or real-time use.
*   **Causal Normalization:** Detail the implementation of a rolling-window `min/max` normalization scheme. This must be computationally efficient for a sliding window.
*   **Adaptive Binning Rule:** Provide an in-depth analysis of the **Freedman-Diaconis rule** for determining the optimal number of histogram bins. The report must justify why this rule is theoretically superior to alternatives like Sturges' or Scott's rule for the typically non-Gaussian, fat-tailed distributions found in financial data. Deliver a TypeScript function `freedmanDiaconisBins(dataWindow: number[]): number` that computes the optimal bin count for a given window of data.

---

### **Section 2: Dynamic State and Parameter Estimation**

**Objective:** To implement a robust, non-linear state-space filter to dynamically estimate the time-varying parameters `k(t)` and `F(t)` from the system's equation of motion.

**Requirement:** A complete technical guide to implementing a Particle Filter for online parameter estimation in TypeScript.
*   **State-Space Model Formulation:** Provide a formal definition of the state-space model. This must include:
    *   The augmented state vector: `x_t = [price_t, velocity_t, k_t, F_t]^T`.
    *   The non-linear state transition function `x_{t+1} = f(x_t)` based on a discrete-time version of the system's equation of motion.
    *   The linear measurement function `z_t = h(x_t)`, where only price is observed.
*   **Particle Filter Implementation:** Deliver a production-ready, object-oriented TypeScript class for a generic Particle Filter. The class must include clear, robust, and well-documented methods for:
    *   `predict()`: Propagating particles according to the state transition model and process noise.
    *   `update()`: Calculating particle likelihoods and updating weights based on new measurements.
    *   `estimate()`: Calculating the weighted mean and covariance of the particle cloud to provide a final state estimate and an uncertainty measure.
*   **Resampling Strategy:** The filter's `resample()` method must specifically implement **Systematic Resampling**. The report must include an explanation of why this method is generally preferred over multinomial resampling for reducing Monte Carlo variation.
*   **Constraint Handling:** Provide a rigorous explanation and code examples for enforcing physical constraints on the parameters (e.g., ensuring stiffness `k > 0`). This should discuss the use of proposal distributions or state-space transformations (e.g., estimating `log(k)` instead of `k`).

---

### **Section 3: Unsupervised Market Regime Discovery**

**Objective:** To replace the current heuristic classifier with a data-driven model that autonomously discovers market regimes and provides a probabilistic classification.

**Requirement:** A report on implementing Gaussian Mixture Models (GMMs) for financial time-series clustering in a JavaScript/TypeScript environment.
*   **Library Survey:** Conduct a comparative review of mature JavaScript/TypeScript libraries that provide GMM functionality (e.g., equivalents to Python's `scikit-learn`). The comparison should be based on performance, API quality, maintenance status, and feature completeness. If no suitable library exists, specify the requirements for a from-scratch implementation.
*   **Model Selection:** Detail the mathematical formula for the **Bayesian Information Criterion (BIC)**. Provide a clear TypeScript/JavaScript implementation demonstrating how to fit a series of GMMs with different component counts and use the BIC scores to programmatically select the optimal number of clusters (regimes).
*   **Probabilistic Classification:** Provide a code example showing how to use the final, fitted GMM to generate a **probabilistic classification** (i.e., the `predict_proba` output) for a new, unseen state vector.
*   **GMM vs. HMM Analysis:** Include a concise but rigorous critique comparing the suitability of Gaussian Mixture Models versus **Hidden Markov Models (HMMs)** for this specific task. The analysis must highlight that HMMs explicitly model the **persistence and transition probabilities** between regimes, a key feature of financial markets that GMMs ignore.

---

### **Section 4: Robust Estimation of the Temperature Metric**

**Objective:** To replace the numerically unstable estimation of the Temperature metric `Θ` with a robust and reliable method for differentiating noisy signals.

**Requirement:** A technical report on robust numerical differentiation methods.
*   **Critique of Current Method:** Begin with a formal explanation of why the current method—a simple linear regression on the first-differences of two noisy time series—is statistically unstable and acts as a high-pass filter that amplifies noise.
*   **Analysis of Alternatives:** Provide a detailed analysis of superior methods for estimating the derivative `d(Volume)/d(Entropy)`, with a strong focus on:
    *   **Local Polynomial Regression (e.g., LOESS/LOWESS):** Explain how fitting a local polynomial to a window of `(Entropy, Volume)` data points and analytically differentiating the resulting polynomial provides a smoothed, robust estimate of the local derivative.
    *   **Savitzky-Golay Differentiator:** Detail how a Savitzky-Golay filter can be configured to directly output a smoothed derivative, and compare its strengths and weaknesses to the local regression approach.
*   **Implementation:** Provide a production-ready TypeScript/JavaScript function implementing the most highly recommended method, including a clear justification for the choice.
