A Rigorous Critique of the kinētikós entropḗ State-Space Model




Part I: An Assessment of the Conceptual Framework


This report provides a critical analysis of the kinētikós entropḗ financial analysis tool. The evaluation is grounded in established principles of quantitative finance, econophysics, and statistical methodology. The analysis proceeds in four parts: an assessment of the core conceptual framework, a mathematical deconstruction of the state-vector components, an examination of systemic implementation flaws, and a critique of the proposed future roadmap.


1.1 The Market as a Physical System: A Contested Analogy


The foundational premise of kinētikós entropḗ is the modeling of a financial market as a physical system governed by mechanical and thermodynamic laws. This approach places the tool squarely within the domain of econophysics, an interdisciplinary field that applies theories from physics to economic phenomena.1 Econophysics has a history of searching for "stylized facts"—universal empirical regularities in economic data, such as the power-law "fat tails" in asset return distributions and the Pareto distribution of wealth.2 The
kinētikós entropḗ model, with its four dimensions, can be seen as an attempt to define a new set of stylized facts or state variables for describing market behavior.
However, this approach is not without significant academic controversy. A primary critique leveled against econophysics is a perceived lack of engagement with established economic theory, a resistance to adopting more robust statistical methods developed within econometrics, and a foundational belief in universal laws governing what are, fundamentally, complex adaptive systems of interacting human agents.2
This brings to light a philosophical divide in the field. Many econophysics models are "bottom-up," using agent-based simulations to show how macroscopic behavior, like market crashes, can emerge from the interactions of individual, boundedly rational agents.2 In contrast,
kinētikós entropḗ employs a "top-down" methodology. It imposes a physical law—a Lagrangian equation of motion—directly onto the aggregate price series. This is a profound conceptual choice. It presupposes that the market as a whole can be treated as a monolithic physical object with properties like "stiffness" and "mass," rather than as an "ecology of interacting adaptive agents".5 This choice makes the model vulnerable to the most salient criticism of the top-down econophysical approach: it largely ignores the reflexive, non-ergodic, and agent-driven nature of markets. A physical object does not alter its governing laws based on observation or changing sentiment; a market, composed of learning and reacting agents, fundamentally does. This disanalogy represents a primary conceptual risk to the model's validity.


1.2 The Financial Lagrangian: A Novel but Unconventional Formulation


In mainstream finance and economics, the Lagrangian formalism is a standard and powerful tool for solving dynamic optimization problems under constraints.7 It is typically employed as an alternative to dynamic programming or solving the Bellman equation, for instance, to find the optimal consumption path for an agent seeking to maximize utility over time subject to a budget constraint.7 The general form of such problems involves optimizing a function subject to a stochastic differential equation (SDE) like
dx=f(x,u)dt+S(x,u)dz, where x is the state and u is the control variable.7
The Lagrangian proposed in kinētikós entropḗ, L=T−V=0.5⋅m⋅(velocity)2−(0.5⋅k⋅(p−peq​)2−F⋅p), is a direct transposition from classical mechanics. It is not derived from an underlying economic principle, such as no-arbitrage conditions or utility maximization. Instead, the potential energy V(p) and kinetic energy T have been postulated by analogy. This represents a significant departure from the conventional use of Lagrangians in finance, where the formalism emerges as a method to solve a well-posed optimization problem rather than being asserted a priori as the description of the system's dynamics.
This reversal of logic raises a critical, unanswered question: What optimization problem, if any, is the market solving such that its dynamics are described by this specific Lagrangian? Without a clear economic basis for this formulation, the model remains a descriptive analogy rather than an explanatory theory. This makes the interpretation of its parameters difficult. While k is labeled "stiffness," its connection to concrete economic concepts like risk aversion, the speed of arbitrage, or market liquidity is purely heuristic and lacks the theoretical grounding of parameters found in established financial models. For example, the mean-reversion speed θ in the Ornstein-Uhlenbeck process has a clear and direct interpretation backed by extensive theory.9 The parameters of this Lagrangian lack such rigor.


1.3 On Analogy vs. Isomorphism: The "Quantum Econophysics" Debate


The questions surrounding the kinētikós entropḗ model echo a similar debate within the sub-field of "quantum econophysics," which applies the mathematical formalism of quantum mechanics to financial markets.10 These quantum models have demonstrated remarkable empirical success in fitting financial data. However, critics rigorously question "what is really quantum" about them, arguing that they are "purely phenomenological" models. The formal analogy is successful, but it does not imply that the market is a quantum system or that traders exhibit properties like superposition or entanglement. The models work, but the asserted analogy between the physical and financial systems lacks a concrete, causal connection.10
This same critique applies with equal force to the classical mechanics analogy used here. One must ask, "What is truly mechanical about this model?" Is there a conserved quantity corresponding to the Hamiltonian (H=T+V)? Does the market's trajectory through state-space truly follow a path of least action? The burden of proof rests on the model's author to demonstrate that this analogy is more than a convenient and visually interesting way to re-package time-series data. Without this deeper justification, kinētikós entropḗ risks being categorized as a phenomenological model. While it may provide a novel framework for visualizing data and classifying market regimes, it cannot claim to be an explanatory model of why markets behave in this manner. This fundamentally limits its scientific contribution and, more practically, makes it difficult to trust its output when market conditions shift into a state not observed in the historical data used for parameter tuning.


Part II: Deconstruction and Mathematical Critique of the State-Vector


This section provides a detailed mathematical and statistical critique of the four dimensions of the state vector Ψ(t)=⟨P,M,E,Θ⟩. The analysis focuses on the robustness of their definitions and the stability of their estimation methods as implemented in the provided code.11


2.1 Dimension 1: Potential (P) — A Brittle Model of Mean Reversion


The Potential dimension is derived from a potential energy function V(p)=0.5⋅k⋅(p−peq​)2−F⋅p, which describes a forced harmonic oscillator. This is intended to model mean-reverting behavior. The canonical model for continuous-time mean reversion in finance is the Ornstein-Uhlenbeck (OU) process, described by the SDE: dXt​=θ(μ−Xt​)dt+σdWt​.9 A comparison reveals significant weaknesses in the
kinētikós entropḗ formulation and estimation.
The code analysis reveals that the equilibrium price, peq​, is calculated as a simple moving average (SMA) of the smoothed price.11 This is a lagging and overly simplistic estimate of the true long-term mean,
μ. More robust methods, such as using an Exponentially Weighted Moving Average (EWMA) or estimating μ directly via maximum likelihood estimation of the OU process, are standard practice.9
The model's most critical mathematical flaw lies in the estimation of the "stiffness" k and "force" F. These parameters are estimated via a rolling Ordinary Least Squares (OLS) regression based on the equation m⋅a=F−k⋅(p−peq​), where a is the price acceleration.11 This method is fundamentally unsuitable for financial data for several reasons:
* Non-Stationarity: Financial time series are famously non-stationary, exhibiting time-varying means and variances. Applying OLS to non-stationary data is known to produce spurious correlations and unreliable parameter estimates.11
* Noise Amplification: The price_acceleration term is a second derivative of price. Taking derivatives of a noisy signal like price dramatically amplifies the noise, making the regression's input data extremely erratic and unreliable, even after the initial Savitzky-Golay smoothing.11
* Model Misspecification: The model is arguably over-parameterized and self-contradictory. It attempts to fit both a mean-reverting force (the k term) and a linear trend force (the F term) simultaneously within a rolling window. The more parsimonious and standard OU model captures mean-reversion without needing an explicit external "force" term. This internal tension between trending and reverting forces likely leads to highly unstable estimates for k and F as the regression struggles to fit two opposing concepts to the same local data.
Consequently, the Potential metric is built on a mathematically fragile foundation. The complexity of the Lagrangian formulation has not yielded a more robust model of mean reversion than the standard OU process; on the contrary, it has produced one that is more complex, less stable, and more difficult to interpret. The proposal in the future roadmap to use Particle Filters to estimate these parameters is a tacit acknowledgment of this severe weakness.
Feature
	kinētikós entropḗ (Potential)
	Ornstein-Uhlenbeck (OU) Process
	Core Concept
	Mechanical analogy of a forced harmonic oscillator.
	Stochastic process with linear drift towards a mean.
	Mathematical Form
	V(p)=0.5k(p−peq​)2−Fp
	dXt​=θ(μ−Xt​)dt+σdWt​
	Key Parameters
	Stiffness (k), Force (F), Equilibrium (peq​)
	Reversion Speed (θ), Mean (μ), Volatility (σ)
	Stochastic Nature
	Deterministic potential function applied to smoothed data.
	Inherently stochastic via the Wiener process term dWt​.
	Estimation Method
	Rolling OLS regression on price derivatives (unstable).
	Maximum Likelihood Estimation (MLE) or regression on discretized SDE (more robust).
	Primary Use Case
	Quantifying "tension" relative to a lagging SMA.
	Modeling interest rates, pairs trading spreads, volatility. 9
	

2.2 Dimension 2: Momentum (M) — A Heuristic for Market Impact


The Momentum dimension is defined as kinetic energy, T=0.5⋅m⋅(price_velocity)2, where "market mass" m is proxied by the ratio volume/price. This is intended to represent the asset's inertia.
While intuitively appealing, the proxy m=volume/price lacks a rigorous theoretical foundation in market microstructure. This ratio is essentially a measure of the number of shares traded per dollar of asset value, which quantifies the breadth of participation. However, true market "inertia" is more accurately described by the concept of liquidity—the market's capacity to absorb order flow without significant price dislocation.16
Standard market impact models, which seek to quantify this effect, are far more sophisticated. They typically model impact as a concave function of trade size, often using a square-root relationship that incorporates the asset's volatility (σ) and its average daily volume (ADV): Impact∝σ⋅V/ADV​.18 These models recognize that a high-volume trade in a very liquid market (e.g., near the market open or close) may have less price impact than a medium-volume trade in an illiquid market (e.g., mid-day).21 The
kinētikós entropḗ "mass" parameter m fails to make this crucial distinction. It treats all volume as having the same weight, scaled only by the current price level, thereby conflating raw participation with the market's absorptive capacity. A more robust measure of inertia would need to incorporate metrics like the bid-ask spread, order book depth, or the ADV, as is standard in the field of transaction cost analysis.


2.3 Dimension 3: Entropy (E) — A Sound Concept with Flawed Implementation


The use of Shannon Entropy to measure market disorder is conceptually sound. Within information theory and its application to finance, low entropy is correctly associated with ordered, predictable states (like a strong trend), while high entropy signifies randomness, uncertainty, and a market that is behaving consistently with the Efficient Market Hypothesis.22 The choice to calculate entropy on the distribution of price velocity, rather than price itself, is also a sophisticated and appropriate design choice.
However, the implementation of this concept is critically flawed by look-ahead bias. The analysis of the calculateRollingLagrangianEntropy function reveals that it determines the bin widths for its histogram by first finding the globalMin and globalMax of the entire velocity time series being analyzed.11 This means that the entropy calculation for a data point in, for example, the first month of a year-long analysis is being performed using bins that are defined by the most extreme price velocities that will occur many months in the future. In a real-time, causal system, this information would be unknowable. This flaw creates an artificially stable and smooth entropy signal, giving the illusion of a clear "predictive climate." A causal implementation, where bin boundaries are determined only by a rolling window of past data, would produce a much noisier and more challenging signal to interpret.
Furthermore, the calculation is highly sensitive to the numBins hyperparameter, which is set to a fixed value. Too few bins will cause a loss of information, while too many bins in a rolling window with a fixed number of data points can lead to statistical bias and an overestimation of entropy.22 The lack of an adaptive binning strategy (e.g., using Sturges' formula or the Freedman-Diaconis rule) makes the metric less robust.


2.4 Dimension 4: Temperature (Θ) — A Novel but Speculative and Unstable Metric


The Temperature dimension, defined as Θ∝(dE/dVolume)−1, is the most conceptually novel component of the model. The idea that market fragility (Θ) can be quantified as the sensitivity of the system's disorder (E) to capital flows (Volume) is intellectually compelling. It suggests a "hot" market is one where a small injection of volume can cause a large, disproportionate increase in randomness and unpredictability, a state that intuitively precedes a phase transition like a crash or a volatility spike.5
While the concept of "market temperature" exists in the econophysics literature, it is often used as an analogy for volatility or the variance of a statistical distribution, and no reviewed source uses this specific derivative-based formulation.1 This definition appears to be an original contribution.
Unfortunately, the brilliance of the concept is completely undermined by its numerical implementation. The derivative dE/dVolume is estimated via a rolling linear regression of the first differences of entropy against the first differences of volume.11 This involves performing a regression on two highly noisy, derived signals. This method is notoriously unstable. The calculation is susceptible to division-by-near-zero errors when the variance of entropy changes is low, leading to extreme and meaningless values for
Θ. The resulting Temperature signal is likely to be the least reliable and most volatile of the four dimensions, rendering it practically unusable in its current form. The future roadmap critically fails to address this instability, focusing on improving other parameters while leaving the flawed estimation of Θ untouched.


Dimension
	Critique of v1.0 Implementation
	Primary Flaw
	Recommendation for v2.0
	Potential (P)
	Uses a complex, over-parameterized model estimated with an unstable OLS regression on non-stationary, noisy data derivatives.
	Statistical Invalidity: OLS regression is inappropriate for the given data, leading to unreliable parameter estimates for k and F.
	Replace OLS with a robust state-space filtering method (e.g., Particle Filter) to estimate time-varying k(t) and F(t).
	Momentum (M)
	"Market mass" proxy (m=volume/price) is a crude heuristic that ignores established principles of market liquidity and impact.
	Conceptual Oversimplification: Conflates raw trading volume with the market's capacity to absorb it (liquidity).
	Re-formulate "mass" to incorporate standard liquidity metrics like bid-ask spread, order book depth, or Average Daily Volume (ADV).
	Entropy (E)
	Entropy bins are defined using the global min/max of the entire dataset, introducing severe look-ahead bias. The number of bins is fixed.
	Acausality: Uses future information to calculate a past state, making the metric invalid for backtesting or real-time use.
	Implement a causal, rolling-window normalization for binning. Explore adaptive binning rules (e.g., Freedman-Diaconis).
	Temperature (Θ)
	A novel and interesting concept (fragility = dE/dVolume) is estimated using a highly unstable regression on noisy first-difference data.
	Numerical Instability: The derivative estimation method is extremely sensitive to noise and prone to producing erratic, unreliable outputs.
	Research and implement a more robust numerical differentiation technique or re-formulate the Temperature metric entirely to be more statistically stable.
	

Part III: Systemic Flaws in Implementation and Visualization


This section addresses systemic issues that affect the tool as a whole, moving beyond the individual components to the integrity of the entire analytical pipeline and its presentation to the user.


3.1 The Pervasiveness of Look-Ahead Bias: A Fatal Flaw in v1.0


The utility of any quantitative tool for financial analysis rests on its causal integrity. A model used for backtesting or real-time decision-making must not, under any circumstances, use information from the future to analyze the past or present. The kinētikós entropḗ tool, in its v1.0 implementation, violates this principle in multiple, severe ways, rendering it invalid for any practical financial application.
The user's documentation correctly identifies one source of this bias: the RegimeClassifier uses the entire historical dataset to calculate percentile ranks, meaning a classification in the past is influenced by the range of values that will occur in the future. While this is a valid critique, it pales in comparison to two far more fundamental and unacknowledged sources of bias embedded deep within the calculation pipeline.11
1. Acausal Data Smoothing (Savitzky-Golay Filter): The very first step in the data processing pipeline is the application of a Savitzky-Golay (SG) filter to smooth the price data and calculate its first and second derivatives (velocity and acceleration).27 The provided code implements a standard,
symmetric SG filter. This means that to calculate the smoothed value at time t, the algorithm uses a window of data points from t−n to t+n. It explicitly uses n data points from the future.28 This is a catastrophic flaw. It means that every single subsequent calculation—
Potential, Momentum, Entropy, and Temperature—is derived from non-causal, future-looking data.
2. Acausal Entropy Binning: As detailed in Part II, the entropy calculation defines its discretization bins based on the global minimum and maximum price velocity observed over the entire analysis period. This is another severe form of look-ahead bias that invalidates the entropy metric.11
The documentation describes the tool's output as a "god-like view of history." This is an accurate description of the visual output, but it misattributes the cause. The clean, smooth trajectories seen in the 3D chart are not an inherent property of the market's state-space revealed by a powerful model. They are an artifact of a broken analytical process. The tool is not a perfect telescope for viewing the past; it is a flawed one that creates artificially clear images by using information it should not possess. The "Causal View" proposed in the roadmap is not an alternative feature; it is the only methodologically correct view, and its implementation would require a complete overhaul of the core calculation engine.


Look-Ahead Bias Source
	Function(s) in Code
	Impact on Model
	Required Mitigation
	Percentile Normalization
	RegimeClassifier, getPercentileRank
	Classification and visualization (dials, radar) for any point in time are based on the full range of data, including the future.
	Implement rolling-window percentile ranking. The rank at time t should be based only on the distribution of data up to time t.
	SG Filter Smoothing & Derivatives
	savitzkyGolay
	The core smoothed price, velocity, and acceleration are non-causal. This invalidates every subsequent metric (P, M, E, Θ) for backtesting or real-time use.
	Replace the symmetric SG filter with a one-sided, causal version that only uses past and present data points for its polynomial fit.
	Entropy Binning
	calculateRollingLagrangianEntropy
	The definition of "disorder" at time t is dependent on the most extreme price velocities that will occur in the future, making the entropy measure non-causal.
	Replace global min/max with a rolling-window min/max. Bin boundaries at time t must be based only on the distribution of velocities within the lookback window ending at t.
	

3.2 Dashboard and Visualization Efficacy


The user interface, as seen in the provided images, presents a modern and visually appealing dashboard. The use of Plotly.js for the 4D state-space chart and other components for the dials and radar chart is effective. However, there are questions regarding the efficacy of the information design.
The central visualization—the 4D trajectory—is striking but presents significant interpretability challenges. A 3D scatter plot where a fourth dimension is encoded as color is notoriously difficult for the human brain to process accurately. Identifying the specific geometric patterns described in the documentation, such as an "inward spiral" for consolidation or a path's curvature, is highly subjective and requires extensive user training. It is not immediately clear that this complex visualization offers more actionable insight than four separate, well-designed time-series charts of P, M, E, and Θ.
Furthermore, there is a mismatch between the dynamic nature of the main chart and the static nature of the supporting analytics. The trajectory tells a story over time, containing rich information in its shape, curvature, and velocity. Yet, the rest of the dashboard—the dials, the radar chart, and the AI analysis—focuses exclusively on the single, final data point of the selected period. The tool provides no quantitative analysis of the path's geometry itself. This is a significant missed opportunity. The system could be calculating metrics like "trajectory torsion," "rate of convergence," or "path volatility," which would transform the subjective visual interpretation into objective, quantifiable data.


3.3 The AI Analyst as a Narrative Engine


The "AI Market Analysis" feature, upon inspection of the implementation code, is not an AI analyst but a natural language report generator.11 The prompt provided to the generative AI model simply feeds it the four final state-vector values (or their percentile ranks) and asks it to write a paragraph based on the predefined descriptions of each dimension. The AI is not performing any analysis on the time series, nor is it identifying patterns in the trajectory. It is merely translating four numbers into prose.
By branding this feature as an "AI Analyst," the tool risks creating a sense of anthropomorphic overconfidence in its users. A user may place undue trust in the generated narrative, believing a higher-level intelligence has synthesized the data and uncovered novel insights. In reality, the output is a deterministic rephrasing of the dashboard's state. To manage user expectations and maintain intellectual honesty, this component should be re-labeled as a "State-Vector Summary" or "Automated Report." True AI analysis would involve a model trained to recognize patterns directly from the time-series data or the trajectory's geometric properties, a far more complex and powerful undertaking.


Part IV: A Roadmap Toward Academic and Practical Viability


The provided future roadmap contains several well-conceived ideas that would significantly improve the model. However, it also has critical omissions. This section provides a critique and expansion of that roadmap.


4.1 Tier 1 (Mandatory): Achieving Causal Integrity


The proposal to implement a "Causal View" free of look-ahead bias is correctly identified as a top priority. This is not merely a new feature; it is the essential, non-negotiable first step to making kinētikós entropḗ a valid analytical tool. This requires a complete rewrite of the core calculation engine to eliminate all sources of look-ahead bias identified in Part III. Specifically, the SG filter must be replaced with a one-sided, causal implementation, and all normalization procedures (for entropy binning and percentile ranking) must be converted to use rolling, backward-looking windows.


4.2 Tier 2 (Essential): Robust Dynamic Parameter Estimation


The proposal to replace the unstable OLS estimation of k and F with advanced statistical filters is an excellent and necessary evolution. This directly addresses the primary mathematical weakness of the Potential dimension. Particle Filters (also known as Sequential Monte Carlo methods) are exceptionally well-suited for this task. They are specifically designed for online parameter estimation in the non-linear, non-Gaussian state-space models that characterize financial markets.29 While computationally intensive, they provide a rigorous framework for estimating time-varying parameters,
k(t) and F(t).
As the roadmap correctly notes, this enhancement would transform the parameters themselves into powerful new signals. The evolution of the market's "stiffness" k(t) or its "trend force" F(t) could provide profound insights into the changing character of market dynamics, such as the strengthening of mean-reverting forces or the exhaustion of a trend. This moves the model from a static snapshot to a truly adaptive system.


4.3 Tier 3 (Advancement): From Heuristics to Probabilistic Classification


The plan to replace the current rule-based RegimeClassifier with an unsupervised machine learning algorithm like a Gaussian Mixture Model (GMM) is a significant step toward analytical maturity. GMMs are a standard and powerful technique for unsupervised market regime detection.33 They operate by assuming the data is generated from a mixture of several distinct Gaussian distributions, each corresponding to a unique market regime.
This approach offers two key advantages over the current heuristic classifier. First, it would allow the system to discover the natural, emergent regimes present in the 4D state-space data, free from human pre-conceptions about what those regimes should be. This could reveal novel or hybrid market states that do not fit neatly into classic labels like "bull trend" or "consolidation." Second, a GMM provides a probabilistic classification (e.g., "70% probability of being in Regime A, 30% in Regime B"), which is a far more realistic and nuanced representation of market ambiguity than the current binary classification.33 This elevates the tool from a simple rules-based system to a data-driven discovery engine.


4.4 Missing Tier 4 (Critical): Re-formulating and Stabilizing the Temperature Metric


The current roadmap contains a glaring omission: it offers no solution for the extreme numerical instability of the Temperature metric. As the most conceptually original part of the model, fixing its implementation should be a top priority. A new tier must be added to the roadmap dedicated to this task. This would involve two potential paths:
   1. Robust Estimation: Researching and implementing more sophisticated and robust numerical differentiation techniques to estimate the derivative dE/dVolume. This could involve using advanced Kalman filters or fitting local parametric models that are less sensitive to the noise in the input signals.
   2. Conceptual Re-formulation: Acknowledging that the derivative of two noisy signals is an inherently unstable problem and re-formulating the Temperature concept to be more statistically tractable. For example, fragility could be defined via the conditional variance of entropy during periods of high volume, or through a different thermodynamic analogy that does not rely on a point-estimate of a noisy derivative.


4.5 Missing Tier 5 (Essential): A Framework for Rigorous Validation


Finally, the roadmap focuses entirely on improving the model's features but lacks a plan for validating its utility. A model, no matter how sophisticated, is useless without empirical proof of its efficacy. The final tier of a credible development plan must be the creation of a rigorous validation framework. This framework should include:
   1. Hyperparameter Sensitivity Analysis: A systematic study of how the model's output and regime classifications change in response to its numerous free parameters (all window sizes, SG polynomial order, entropy bin counts). This is crucial for understanding the model's robustness.
   2. Out-of-Sample Testing: Once a fully causal version is built, it must be tested on data that was not used in any part of the model's development or parameter tuning process.
   3. Statistical Backtesting: This is the ultimate test of economic value. A simple, systematic trading strategy should be built based on the model's regime classifications (e.g., "go long during 'Stable Bull Trend' regimes, move to cash during 'Chaotic Indecision' regimes"). The performance of this strategy (e.g., Sharpe ratio, maximum drawdown, risk-adjusted returns) must be rigorously compared against a passive buy-and-hold benchmark. Only through such a process can one determine if the complex state-space constructed by kinētikós entropḗ provides any genuine, exploitable information about future market behavior.
Works cited
   1. Econophysics - Wikipedia, accessed on June 30, 2025, https://en.wikipedia.org/wiki/Econophysics
   2. ECONOPHYSICS AND THE COMPLEXITY OF FINANCIAL MARKETS - PhilSci-Archive, accessed on June 30, 2025, https://philsci-archive.pitt.edu/00003851/01/EconoCompFinal.pdf
   3. Comments and Criticisms: Econophysics and Sociophysics | Request PDF - ResearchGate, accessed on June 30, 2025, https://www.researchgate.net/publication/226046108_Comments_and_Criticisms_Econophysics_and_Sociophysics
   4. Econophysics review: I. Empirical facts: Quantitative Finance, accessed on June 30, 2025, https://www.tandfonline.com/doi/abs/10.1080/14697688.2010.539248
   5. What's the use of “econo-physics”? | by Mark Buchanan - Medium, accessed on June 30, 2025, https://medium.com/the-physics-of-finance/whats-the-use-of-econo-physics-dae83e0d7d8a
   6. [0909.1974] Econophysics: Empirical facts and agent-based models - arXiv, accessed on June 30, 2025, https://arxiv.org/abs/0909.1974
   7. the lagrange method of optimization in finance - Princeton University, accessed on June 30, 2025, https://www.princeton.edu/~erp/ERParchives/archivepdfs/M369.pdf
   8. lagrangians in economics: constrained optimization - YouTube, accessed on June 30, 2025, https://m.youtube.com/watch?v=Ob56YXIV3rM
   9. Ornstein-Uhlenbeck Process for Mean Reversion - QuestDB, accessed on June 30, 2025, https://questdb.com/glossary/ornstein-uhlenbeck-process-for-mean-reversion/
   10. What Is Really Quantum in Quantum Econophysics? | Philosophy of Science, accessed on June 30, 2025, https://www.cambridge.org/core/journals/philosophy-of-science/article/what-is-really-quantum-in-quantum-econophysics/72BD63D409B4B073EC0076CAA2267643
   11. kenetikos html fin.txt
   12. Ornstein-Uhlenbeck Simulation with Python | QuantStart, accessed on June 30, 2025, https://www.quantstart.com/articles/ornstein-uhlenbeck-simulation-with-python/
   13. Basics of Statistical Mean Reversion Testing | QuantStart, accessed on June 30, 2025, https://www.quantstart.com/articles/Basics-of-Statistical-Mean-Reversion-Testing/
   14. Mean Reversion Cloud (Ornstein-Uhlenbeck) // AlgoFyre - TradingView, accessed on June 30, 2025, https://www.tradingview.com/script/39Nkoycz-Mean-Reversion-Cloud-Ornstein-Uhlenbeck-AlgoFyre/
   15. Ornstein–Uhlenbeck process - Wikipedia, accessed on June 30, 2025, https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process
   16. Market impact - Wikipedia, accessed on June 30, 2025, https://en.wikipedia.org/wiki/Market_impact
   17. Market impact models and optimal trade execution - Homepages of UvA/FNWI staff, accessed on June 30, 2025, https://staff.fnwi.uva.nl/a.khedher/winterschool/9slidesSchied.pdf
   18. Market Impact Models | QuestDB, accessed on June 30, 2025, https://questdb.com/glossary/market-impact-models/
   19. The market impact of large trading orders: Correlated order flow, asymmetric liquidity and efficient prices - Berkeley Haas, accessed on June 30, 2025, https://haas.berkeley.edu/wp-content/uploads/hiddenImpact13.pdf
   20. Market impact of orders, and models that predict it | WeAreAdaptive, accessed on June 30, 2025, https://weareadaptive.com/trading-resources/blog/market-impact-of-orders-and-models-that-predict-it/
   21. Market Impact Models | Pretty Quant, accessed on June 30, 2025, https://www.prettyquant.com/post/2022-09-03-market-impact-models/
   22. Shannon Entropy - QuestDB, accessed on June 30, 2025, https://questdb.com/glossary/shannon-entropy/
   23. Entropy as a Tool for the Analysis of Stock Market Efficiency During Periods of Crisis - MDPI, accessed on June 30, 2025, https://www.mdpi.com/1099-4300/26/12/1079
   24. The Power of Information Theory in Trading: Beyond Shannon's Entropy - Spheron's Blog, accessed on June 30, 2025, https://blog.spheron.network/the-power-of-information-theory-in-trading-beyond-shannons-entropy
   25. The Role of Temperature in Economic Exchange - An Empirical Analysis - IDEAS/RePEc, accessed on June 30, 2025, https://ideas.repec.org/a/cbk/journl/v4y2015i3p65-89.html
   26. Thermodynamic Analysis of Financial Markets: Measuring Order Book Dynamics with Temperature and Entropy - MDPI, accessed on June 30, 2025, https://www.mdpi.com/1099-4300/26/1/24
   27. Savitzky–Golay filter - Wikipedia, accessed on June 30, 2025, https://en.wikipedia.org/wiki/Savitzky%E2%80%93Golay_filter
   28. Introduction to the Savitzky-Golay Filter: A Comprehensive Guide (Using Python) - Medium, accessed on June 30, 2025, https://medium.com/pythoneers/introduction-to-the-savitzky-golay-filter-a-comprehensive-guide-using-python-b2dd07a8e2ce
   29. Particle Filtering-based Maximum Likelihood Estimation for Financial Parameter Estimation - CORE, accessed on June 30, 2025, https://core.ac.uk/download/pdf/77002125.pdf
   30. Parameter Learning and Change Detection Using a Particle Filter with Accelerated Adaptation - MDPI, accessed on June 30, 2025, https://www.mdpi.com/2227-9091/9/12/228
   31. Particle Filters and Bayesian Inference In Financial Econometrics 1 Introduction, accessed on June 30, 2025, https://faculty.mccombs.utexas.edu/carlos.carvalho/teaching/lopes-tsay-2010.pdf
   32. On Particle Methods for Parameter Estimation in State-Space Models - arXiv, accessed on June 30, 2025, https://arxiv.org/pdf/1412.8695
   33. Gaussian Mixture Models (GMM) — AI Meets Finance: Algorithms Series | by Leo Mercanti, accessed on June 30, 2025, https://wire.insiderfinance.io/gaussian-mixture-models-gmm-ai-meets-finance-algorithms-series-d97262deadee
   34. Executive summary A Machine Learning Approach to Regime Modeling 10/21 - Two Sigma, accessed on June 30, 2025, https://www.twosigma.com/wp-content/uploads/2021/10/Machine-Learning-Approach-to-Regime-Modeling_.pdf
   35. Classifying market regimes | Macrosynergy, accessed on June 30, 2025, https://macrosynergy.com/research/classifying-market-regimes/